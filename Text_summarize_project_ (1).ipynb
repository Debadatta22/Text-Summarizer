{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f2378ad"
      },
      "source": [
        "# **My Setup**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Gradio for the web interface\n",
        "!pip install gradio --quiet\n",
        "\n",
        "# Install Hugging Face Transformers library\n",
        "!pip install transformers --quiet\n",
        "\n",
        "# Install RAKE-NLTK for keyword extraction\n",
        "!pip install rake-nltk --quiet\n",
        "\n",
        "# Install PDF handling library\n",
        "!pip install pypdf --quiet\n",
        "\n",
        "# Install BeautifulSoup and requests for web scraping\n",
        "!pip install beautifulsoup4 requests --quiet\n",
        "\n",
        "# Install TextStat for readability scoring\n",
        "!pip install textstat --quiet\n",
        "\n",
        "# Install NLTK and download required data\n",
        "!pip install nltk --quiet\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "R1vBKUl1SeQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96adddf8-6432-42bf-d176-3eeb9cb9331f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install gradio\n",
        "!pip install newspaper3k\n",
        "!pip install nltk\n",
        "!pip install sumy\n",
        "!pip install pdfplumber\n",
        "!pip install python-docx\n",
        "!pip install langdetect\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsT2rkVPM8bx",
        "outputId": "9d0aa59b-328d-4cad-8025-54490a8723fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.2.1)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.6.15)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=11d9c4adaa9f3879f3eb8cf02438e1b33715532a6e6909e7d3b31d319ae166c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=9041b29194facd08d3b43b298edb712fd460ef6ccdb02885f0548cdc41fe4b0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=393db7d0750046331b53372ecb2d595a15b11b560453971090be8643ebd04e25\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=edad058049954f842c515d9952be353ac1fe0c19718d79c27bca430438f29a13\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.6.15)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=bfed58c734fcbce1ef6012497764b4cdda863d69d7dc7fca8d9de4a00666e83a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=5905778c9913006a00eda346ee27c5768fc6ba624652d70d1d382a3fdc5422b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.1\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=3035f83d48dbdf55b3e5fd3b3c3ba47318049eb2222a777236da12789a3e8bea\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7a5ed41"
      },
      "source": [
        "%pip install pypdf requests beautifulsoup4 -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa61922c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312115fd-3551-49d8-9e13-2e7c804198ba"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ead4da"
      },
      "source": [
        "%pip install google-auth google-auth-oauthlib google-auth-httplib2 -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73e797e7"
      },
      "source": [
        "%pip install textstat -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "583e3ce6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7143838d-88eb-4da8-ee62-60d3a9cd310f"
      },
      "source": [
        "# Create requirements.txt\n",
        "requirements = [\n",
        "    \"gradio\",\n",
        "    \"transformers\",\n",
        "    \"rake-nltk\",\n",
        "    \"requests\",\n",
        "    \"beautifulsoup4\",\n",
        "    \"pypdf\",\n",
        "    \"torch\", # Often needed for transformers models\n",
        "    \"accelerate\", # Can help with model loading and inference\n",
        "    \"textstat\",\n",
        "    \"nltk\" # Explicitly add nltk\n",
        "]\n",
        "\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    for req in requirements:\n",
        "        f.write(req + \"\\n\")\n",
        "print(\"Created requirements.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e61a30e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e86b3a-a5e7-40d3-b43e-a375dcdf08e9"
      },
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "print(\"Downloading NLTK data (stopwords and punkt)...\")\n",
        "try:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    print(\"NLTK stopwords downloaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK stopwords: {e}\")\n",
        "\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"NLTK punkt tokenizer downloaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK punkt tokenizer: {e}\")\n",
        "\n",
        "print(\"NLTK data download attempts finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data (stopwords and punkt)...\n",
            "NLTK stopwords downloaded.\n",
            "NLTK punkt tokenizer downloaded.\n",
            "NLTK data download attempts finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "437edd97",
        "outputId": "77185c95-258d-4f86-acc7-9d3e5a710d5d"
      },
      "source": [
        "# Install necessary libraries if not already installed\n",
        "%pip install gradio transformers rake-nltk requests beautifulsoup4 pypdf torch accelerate textstat nltk -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c2ed197",
        "outputId": "2d94a919-ee65-4384-9da9-dccfe536b87c"
      },
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "print(\"Downloading NLTK data (stopwords and punkt)...\")\n",
        "try:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    print(\"NLTK stopwords downloaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK stopwords: {e}\")\n",
        "\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"NLTK punkt tokenizer downloaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK punkt tokenizer: {e}\")\n",
        "\n",
        "print(\"NLTK data download attempts finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data (stopwords and punkt)...\n",
            "NLTK stopwords downloaded.\n",
            "NLTK punkt tokenizer downloaded.\n",
            "NLTK data download attempts finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa6027d8",
        "outputId": "a0fe70d1-98d8-4493-a9bc-8e347a9f7d4e"
      },
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "print(\"Downloading NLTK data (stopwords and punkt) explicitly...\")\n",
        "try:\n",
        "    nltk.download('stopwords')\n",
        "    print(\"NLTK stopwords downloaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK stopwords: {e}\")\n",
        "\n",
        "try:\n",
        "    nltk.download('punkt')\n",
        "    print(\"NLTK punkt tokenizer downloaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK punkt tokenizer: {e}\")\n",
        "\n",
        "print(\"NLTK data explicit download attempts finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data (stopwords and punkt) explicitly...\n",
            "NLTK stopwords downloaded.\n",
            "NLTK punkt tokenizer downloaded.\n",
            "NLTK data explicit download attempts finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a76a9195",
        "outputId": "564650e9-0017-477f-def7-34fce76f7595"
      },
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "print(\"Downloading NLTK data: punkt_tab...\")\n",
        "try:\n",
        "    nltk.download('punkt_tab')\n",
        "    print(\"NLTK punkt_tab downloaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK punkt_tab: {e}\")\n",
        "\n",
        "print(\"NLTK punkt_tab explicit download attempt finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data: punkt_tab...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK punkt_tab downloaded.\n",
            "NLTK punkt_tab explicit download attempt finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code:**\n",
        "execute or run the following tab for output or Summarization\n"
      ],
      "metadata": {
        "id": "z1Npn2-UMXjF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "cd7bf46e",
        "outputId": "fd2717a7-0d65-4ffa-d271-bfceb4d5a9ae"
      },
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "from rake_nltk import Rake\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pypdf import PdfReader\n",
        "import io\n",
        "import tempfile\n",
        "import sys\n",
        "import traceback\n",
        "import nltk\n",
        "import textstat\n",
        "import os\n",
        "\n",
        "# Define available models\n",
        "AVAILABLE_SUMMARIZATION_MODELS = {\n",
        "    \"BART Large CNN\": \"facebook/bart-large-cnn\",\n",
        "    \"T5 Base\": \"t5-base\", # Example of another model\n",
        "}\n",
        "\n",
        "AVAILABLE_QA_MODELS = {\n",
        "    \"DistilBERT SQuAD\": \"distilbert-base-uncased-distilled-squad\",\n",
        "    \"BERT Base Cased SQuAD\": \"bert-base-cased-finetuned-squad\", # Example of another model\n",
        "}\n",
        "\n",
        "# Cache models to avoid reloading on each call (already defined)\n",
        "cached_pipelines = {}\n",
        "\n",
        "def extract_text_from_pdf(pdf_file_path):\n",
        "    \"\"\"Extracts text from an uploaded PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        # Ensure the file exists and is accessible\n",
        "        if not pdf_file_path or not isinstance(pdf_file_path, str):\n",
        "            return None, \"Invalid PDF file input.\"\n",
        "        if not tempfile.TemporaryDirectory().name in pdf_file_path and not os.path.exists(pdf_file_path):\n",
        "             # Check if it's a temporary file path from Gradio or a standard path\n",
        "            return None, f\"PDF file not found: {pdf_file_path}\"\n",
        "\n",
        "        reader = PdfReader(pdf_file_path)\n",
        "        if not reader.pages:\n",
        "             return None, \"PDF file contains no readable pages.\"\n",
        "\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "\n",
        "        if not text.strip():\n",
        "             return None, \"No text extracted from the PDF.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading PDF: {e}\\n{traceback.format_exc()}\"\n",
        "    return text, None\n",
        "\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    \"\"\"Extracts text content from a given URL.\"\"\"\n",
        "    text = \"\"\n",
        "    if not url or not isinstance(url, str):\n",
        "        return None, \"Invalid URL input.\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10) # Added timeout\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Improved text extraction: try different strategies\n",
        "        extracted_text = \"\"\n",
        "        # Strategy 1: Find common article/main content tags\n",
        "        main_content = soup.find('article') or soup.find('main')\n",
        "        if main_content:\n",
        "             text_elements = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']) # Added list items\n",
        "             extracted_text = ' '.join(elem.get_text() for elem in text_elements if elem.get_text())\n",
        "\n",
        "        # Strategy 2: If not much found, try extracting text from common block elements\n",
        "        if not extracted_text or len(extracted_text.split()) < 50: # If limited text from main content\n",
        "             block_elements = soup.find_all(['p', 'div', 'blockquote', 'li']) # Added div, blockquote\n",
        "             extracted_text = ' '.join(elem.get_text() for elem in block_elements if elem.get_text())\n",
        "\n",
        "\n",
        "        # Strategy 3: Fallback to getting all text if still not enough\n",
        "        if not extracted_text or len(extracted_text.split()) < 50:\n",
        "             extracted_text = soup.get_text()\n",
        "\n",
        "\n",
        "        text = extracted_text\n",
        "\n",
        "        # Basic cleaning: remove excessive whitespace and potential script/style text\n",
        "        for script_or_style in soup([\"script\", \"style\"]):\n",
        "            script_or_style.extract() # Remove these tags\n",
        "\n",
        "        text = ' '.join(text.split()) # Normalize whitespace\n",
        "\n",
        "        if not text.strip():\n",
        "             return None, \"No meaningful text extracted from the URL.\"\n",
        "\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        return None, f\"Error fetching URL: Request timed out after 10 seconds.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return None, f\"Error fetching URL: {e}\\n{traceback.format_exc()}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error parsing URL content: {e}\\n{traceback.format_exc()}\"\n",
        "    return text, None\n",
        "\n",
        "\n",
        "def get_pipeline(task, model_name):\n",
        "    \"\"\"Gets or creates a cached pipeline for a given task and model.\"\"\"\n",
        "    cache_key = f\"{task}_{model_name}\"\n",
        "    if not model_name:\n",
        "         return None, f\"No model selected for {task}.\"\n",
        "\n",
        "    if cache_key not in cached_pipelines:\n",
        "        try:\n",
        "            print(f\"Loading {task} model: {model_name}\")\n",
        "            # Add device mapping for potentially better performance if GPU available\n",
        "            try:\n",
        "                import torch\n",
        "                device = 0 if torch.cuda.is_available() else -1\n",
        "            except ImportError:\n",
        "                device = -1 # Default to CPU if torch is not available or CUDA is not\n",
        "\n",
        "            if task == \"summarization\":\n",
        "                 cached_pipelines[cache_key] = pipeline(task, model=model_name, device=device)\n",
        "            elif task == \"question-answering\":\n",
        "                 cached_pipelines[cache_key] = pipeline(task, model=model_name, return_all_scores=True, device=device)\n",
        "            else:\n",
        "                 cached_pipelines[cache_key] = pipeline(task, model=model_name, device=device)\n",
        "            print(f\"Successfully loaded {model_name} for {task}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {model_name} for {task}: {e}\\n{traceback.format_exc()}\")\n",
        "            # Remove the failed model from cache if it was partially added\n",
        "            if cache_key in cached_pipelines:\n",
        "                 del cached_pipelines[cache_key]\n",
        "            return None, f\"Error loading model: {model_name}. Details: {e}\"\n",
        "    else:\n",
        "        print(f\"Using cached model: {model_name} for {task}\")\n",
        "    return cached_pipelines.get(cache_key), None\n",
        "\n",
        "\n",
        "def summarize_and_answer_advanced(\n",
        "    text_input_box, file_input, url_input, question, summarization_type,\n",
        "    summary_length_sentences, summary_format,\n",
        "    summarization_model_key, qa_model_key,\n",
        "    max_length, min_length,\n",
        "    history\n",
        "):\n",
        "    \"\"\"\n",
        "    Handles input from textbox, file upload, or URL, summarizes the text,\n",
        "    extracts keywords, and answers questions based on the text, with advanced\n",
        "    summarization options, supporting follow-up questions and confidence scores.\n",
        "    Allows model selection and parameter tuning, and summary format selection.\n",
        "    Calculates and returns output metrics.\n",
        "    Includes basic error handling and prepares outputs for download.\n",
        "    Implements caching for models and handles large text inputs.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    summary = \"\"\n",
        "    keywords = []\n",
        "    error_message = \"\"\n",
        "    new_history = history\n",
        "    download_summary = \"\"\n",
        "    download_qa = \"\"\n",
        "\n",
        "    # Metrics initialization\n",
        "    input_word_count = 0\n",
        "    summary_word_count = 0\n",
        "    compression_rate = 0.0\n",
        "    readability_score = 0.0\n",
        "\n",
        "\n",
        "    MAX_TEXT_SIZE_BYTES = 1 * 1024 * 1024 # 1 MB limit for input text to prevent memory issues\n",
        "\n",
        "    # --- Input Handling ---\n",
        "    # Prioritizing input sources: File > URL > Textbox\n",
        "    if file_input is not None:\n",
        "        # Gradio's File component provides a file path\n",
        "        file_path = file_input # Gradio passes the file path as a string\n",
        "        if file_path: # Ensure file_path is not None or empty\n",
        "            if file_path.lower().endswith('.pdf'):\n",
        "                text, error_message = extract_text_from_pdf(file_path)\n",
        "            elif file_path.lower().endswith('.txt'):\n",
        "                 try:\n",
        "                     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                         text = f.read()\n",
        "                 except Exception as e:\n",
        "                     text = None\n",
        "                     error_message = f\"Error reading text file: {e}\\n{traceback.format_exc()}\"\n",
        "            else:\n",
        "                 text = None\n",
        "                 error_message = \"Unsupported file type. Please upload a .txt or .pdf file.\"\n",
        "        else:\n",
        "             text = None\n",
        "             error_message = \"File input is present but file path is missing.\"\n",
        "\n",
        "\n",
        "    elif url_input:\n",
        "        text, error_message = extract_text_from_url(url_input)\n",
        "\n",
        "    elif text_input_box:\n",
        "        text = text_input_box\n",
        "\n",
        "\n",
        "    # Calculating input word count immediately after getting the text\n",
        "    if text:\n",
        "        input_word_count = len(text.split())\n",
        "\n",
        "    if error_message:\n",
        "         # Ensuring all outputs are returned even with an error\n",
        "         return summary, \"; \".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score\n",
        "\n",
        "    if not text:\n",
        "        error_message = \"Please provide text via the textbox, file upload, or URL.\"\n",
        "        return summary, \"; \".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score\n",
        "\n",
        "    # Checking text size\n",
        "    if sys.getsizeof(text) > MAX_TEXT_SIZE_BYTES:\n",
        "        error_message = f\"Input text size exceeds the limit of {MAX_TEXT_SIZE_BYTES / (1024*1024):.2f} MB. Please provide a smaller text.\"\n",
        "        return summary, \"; \".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score\n",
        "\n",
        "\n",
        "    # Getting the selected summarization model pipeline\n",
        "    summarization_model_name = AVAILABLE_SUMMARIZATION_MODELS.get(summarization_model_key)\n",
        "    summarizer, model_load_error = get_pipeline(\"summarization\", summarization_model_name)\n",
        "    if model_load_error:\n",
        "        error_message += f\"\\n{model_load_error}\"\n",
        "        return summary, \"; \".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score\n",
        "\n",
        "    # Getting the selected QA model pipeline\n",
        "    qa_model_name = AVAILABLE_QA_MODELS.get(qa_model_key)\n",
        "    question_answerer, model_load_error = get_pipeline(\"question-answering\", qa_model_name)\n",
        "    if model_load_error:\n",
        "        error_message += f\"\\n{model_load_error}\"\n",
        "        return summary, \"; \".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score\n",
        "\n",
        "\n",
        "    # --- Summarization and Keyword Extraction ---\n",
        "    try:\n",
        "        if summarization_type == \"Abstractive\" and summarizer:\n",
        "\n",
        "            summarizer_output = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "            if summarizer_output and isinstance(summarizer_output, list) and len(summarizer_output) > 0 and 'summary_text' in summarizer_output[0]:\n",
        "                 summary = summarizer_output[0]['summary_text']\n",
        "            else:\n",
        "                 summary = \"Could not generate abstractive summary.\"\n",
        "                 print(f\"Abstractive summarizer returned unexpected output: {summarizer_output}\")\n",
        "                 if not error_message: # Add to error message only if no prior error\n",
        "                      error_message = \"Abstractive summarizer failed.\"\n",
        "                 else:\n",
        "                      error_message += \"\\nAbstractive summarizer failed.\"\n",
        "\n",
        "\n",
        "        elif summarization_type == \"Extractive\":\n",
        "            # Using NLTK's sent_tokenize for more accurate sentence splitting\n",
        "            try:\n",
        "                sentences = nltk.sent_tokenize(text)\n",
        "                sentences = [s.strip() for s in sentences if s.strip()]\n",
        "                if not sentences:\n",
        "                     summary = \"Could not extract sentences for extractive summary.\"\n",
        "                     if not error_message:\n",
        "                          error_message = \"Extractive summarizer failed.\"\n",
        "                else:\n",
        "                     num_sentences = min(len(sentences), summary_length_sentences)\n",
        "                     summary = '. '.join(sentences[:num_sentences])\n",
        "            except LookupError:\n",
        "                 summary = \"Extractive summarization failed (NLTK punkt missing).\"\n",
        "                 if not error_message: error_message = \"\"\n",
        "                 error_message += \"\\nNLTK Punkt tokenizer not found. Please ensure it's downloaded.\"\n",
        "            except Exception as e:\n",
        "                 summary = \"Error during extractive summarization.\"\n",
        "                 if not error_message: error_message = \"\"\n",
        "                 error_message += f\"\\nError during extractive summarization: {e}\\n{traceback.format_exc()}\"\n",
        "\n",
        "\n",
        "        elif not summarizer:\n",
        "             summary = \"Summarization model not loaded.\"\n",
        "             if not error_message:\n",
        "                  error_message = \"Summarization model not loaded.\"\n",
        "\n",
        "\n",
        "        # Applying summary format\n",
        "        if summary and summary_format == \"Bullet Points\":\n",
        "            # Usng NLTK's sent_tokenize for more accurate sentence splitting for bullet points\n",
        "            try:\n",
        "                sentences_for_bullets = nltk.sent_tokenize(summary)\n",
        "                # Filtering out empty strings and strip whitespace, then add bullet points\n",
        "                bullet_points = [f\"- {s.strip()}\" for s in sentences_for_bullets if s.strip()]\n",
        "                summary = \"\\n\".join(bullet_points)\n",
        "            except LookupError:\n",
        "                 # This error should ideally be caught during initial NLTK download check, but as a fallback\n",
        "                 error_message += \"\\nNLTK Punkt tokenizer not found. Cannot format summary as bullet points.\"\n",
        "                 # summary remains in paragraph format\n",
        "            except Exception as e:\n",
        "                 error_message += f\"\\nError formatting summary as bullet points: {e}\\n{traceback.format_exc()}\"\n",
        "                 # Fallback to original summary if formatting fails\n",
        "                 pass\n",
        "\n",
        "\n",
        "        # Calculation for summary word count (after formatting)\n",
        "        if summary:\n",
        "            summary_word_count = len(summary.split())\n",
        "\n",
        "        # Calculation for compression rate\n",
        "        if input_word_count > 0:\n",
        "            compression_rate = ((input_word_count - summary_word_count) / input_word_count) * 100\n",
        "            compression_rate = max(0.0, compression_rate)\n",
        "        else:\n",
        "             compression_rate = 0.0\n",
        "\n",
        "        # Calculation for readability score\n",
        "        if text:\n",
        "            try:\n",
        "                if len(text.split()) >= 50:\n",
        "                     readability_score = textstat.flesch_reading_ease(text)\n",
        "                else:\n",
        "                     readability_score = -2.0 # Indicate text is too short\n",
        "                     if not error_message: error_message = \"\"\n",
        "                     error_message += \"\\nNote: Readability score may be unreliable for texts shorter than 50 words.\"\n",
        "\n",
        "            except Exception as re:\n",
        "                readability_score = -1.0 # Indicate error\n",
        "                print(f\"Error calculating readability score: {re}\\n{traceback.format_exc()}\")\n",
        "                if not error_message: error_message = \"\"\n",
        "                error_message += f\"\\nError calculating readability score: {re}\"\n",
        "\n",
        "\n",
        "        # Keyword Extraction\n",
        "        try:\n",
        "\n",
        "            try:\n",
        "                nltk.data.find('tokenizers/punkt')\n",
        "                nltk.data.find('corpora/stopwords')\n",
        "            except LookupError:\n",
        "                 print(\"NLTK data not found. Attempting re-download within function...\")\n",
        "                 try:\n",
        "                     import ssl\n",
        "                     try:\n",
        "                         _create_unverified_https_context = ssl._create_unverified_context\n",
        "                     except AttributeError:\n",
        "                         pass\n",
        "                     else:\n",
        "                         ssl._create_default_https_context = _create_unverified_https_context\n",
        "                     nltk.download('punkt', quiet=True)\n",
        "                     nltk.download('stopwords', quiet=True)\n",
        "                     print(\"NLTK data re-downloaded.\")\n",
        "                 except Exception as e:\n",
        "                     print(f\"Error during NLTK re-download: {e}\")\n",
        "\n",
        "\n",
        "            r = Rake()\n",
        "            r.extract_keywords_from_text(text)\n",
        "            keywords = r.get_ranked_phrases()[:10]\n",
        "            if not keywords:\n",
        "                 keywords = [\"No keywords extracted.\"]\n",
        "        except Exception as ke:\n",
        "             keywords = [\"Error extracting keywords. Please check NLTK data.\"]\n",
        "             print(f\"Keyword Extraction Error: {ke}\\n{traceback.format_exc()}\")\n",
        "             if not error_message: error_message = \"\"\n",
        "             error_message += f\"\\nError extracting keywords: {ke}\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        summary = \"Error during summarization or keyword extraction.\"\n",
        "        keywords = [\"Error during processing.\"]\n",
        "        error_message = f\"Processing Error during summarization/keywords: {e}\\n{traceback.format_exc()}\"\n",
        "        return summary, \"; \".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score\n",
        "def download_file(content, filename):\n",
        "    if content:\n",
        "        try:\n",
        "            with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False, suffix=\".txt\", encoding='utf-8') as tmp_file:\n",
        "                tmp_file.write(content)\n",
        "                tmp_file_path = tmp_file.name\n",
        "            return gr.File(value=tmp_file_path, filename=filename, visible=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating temporary file for download: {e}\\n{traceback.format_exc()}\")\n",
        "            return None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def update_download_buttons(summary_content, qa_content):\n",
        "    summary_interactive = bool(summary_content)\n",
        "    qa_interactive = bool(qa_content)\n",
        "    return gr.update(interactive=summary_interactive), gr.update(interactive=qa_interactive)\n",
        "\n",
        "\n",
        "def clear_all():\n",
        "    return (\n",
        "        gr.update(value=\"\"), # text_input\n",
        "        gr.update(value=None), # file_input\n",
        "        gr.update(value=\"\"), # url_input\n",
        "        gr.update(value=\"\"), # question_input\n",
        "        gr.update(value=\"\"), # summary_output\n",
        "        gr.update(value=\"\"), # keywords_output\n",
        "        None, # chatbot\n",
        "        gr.update(value=\"\"), # error_display\n",
        "        [], # conversation_history_state\n",
        "        \"\",\n",
        "        \"\",\n",
        "        gr.update(interactive=False), # download_summary_button\n",
        "        gr.update(interactive=False), # download_qa_button\n",
        "        gr.update(value=\"\"), # input_word_count_output\n",
        "        gr.update(value=\"\"), # summary_word_count_output\n",
        "        gr.update(value=\"\"), # compression_rate_output\n",
        "        gr.update(value=\"\") # readability_score_output\n",
        "    )\n",
        "\n",
        "\n",
        "# Creating the Gradio interface with all features integrated\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Text Processing App\") as iface:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Welcome to our advanced Text Processing App!\n",
        "    Summarize text, extract keywords, and get answers to your questions.\n",
        "    Provide text via the textbox, upload a file (.txt, .pdf), or enter a URL.\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "    # Adding instructions section\n",
        "    gr.Markdown(\"\"\"\n",
        "    ## How to Use:\n",
        "\n",
        "    1.  **Input Text:** Provide the text you want to process by:\n",
        "        *   Typing or pasting it directly into the \"Input Text\" box.\n",
        "        *   Uploading a `.txt` or `.pdf` file using the \"Upload Text File\" option.\n",
        "        *   Entering a URL in the \"Input from URL\" box (the application will attempt to scrape the main text content).\n",
        "        *   *Note: Only one input method is needed. File upload and URL input take precedence over the textbox.*\n",
        "        *   *Input text size is limited to approximately 1 MB to prevent performance issues.*\n",
        "\n",
        "    2.  **Summarization Options:**\n",
        "        *   Choose \"Abstractive\" or \"Extractive\" summarization.\n",
        "        *   For Abstractive, use the sliders to set the desired \"Max Summary Length\" and \"Min Summary Length\" in tokens.\n",
        "        *   For Extractive, use the slider to set the desired \"Summary Length\" in sentences.\n",
        "        *   **Summary Format:** Choose between \"Paragraph\" or \"Bullet Points\".\n",
        "\n",
        "    3.  **Model Selection:**\n",
        "        *   Select your preferred Summarization Model from the dropdown.\n",
        "        *   Select your preferred Question Answering Model from the dropdown.\n",
        "\n",
        "    4.  **Ask Questions:**\n",
        "        *   Enter your questions in the \"Ask a Question(s)\" box. You can ask multiple questions by typing each on a new line.\n",
        "        *   The application will attempt to answer questions based on the provided input text.\n",
        "        *   Follow-up questions are supported as the conversation history is used as additional context.\n",
        "\n",
        "    5.  **Process:** Click the \"Process Text and Questions\" button to run the summarization, keyword extraction, and question answering.\n",
        "\n",
        "    6.  **View Results:**\n",
        "        *   The \"Summarized Text\" box will show the generated summary.\n",
        "        *   The \"Extracted Keywords\" box will list key phrases from the text, separated by semicolons.\n",
        "        *   The \"Conversation History and Answers\" chatbot will display your questions and the model's answers, including a confidence score for each answer.\n",
        "        *   Any messages or errors will appear in the \"Messages\" box.\n",
        "        *   **Output Metrics:** See the calculated metrics below the output areas.\n",
        "\n",
        "    7.  **Download Results:**\n",
        "        *   Click \"Download Summary\" to save the summarized text as a .txt file.\n",
        "        *   Click \"Download Q&A\" to save the conversation history and answers as a .txt file.\n",
        "        *   *Download buttons become active after processing.*\n",
        "\n",
        "    8.  **Clear:** Click the \"Clear All\" button to reset all input and output fields.\n",
        "\n",
        "    ## About the Models:\n",
        "\n",
        "    This application uses transformer models from the Hugging Face library.\n",
        "\n",
        "    *   **Summarization Models:**\n",
        "        *   **BART Large CNN (`facebook/bart-large-cnn`):** A powerful abstractive summarization model fine-tuned on the CNN/DailyMail dataset. It is known for generating coherent and fluent summaries. *Limitations: Can be computationally intensive, may struggle with very long documents (exceeding its context window), and might occasionally hallucinate details not present in the source text.*\n",
        "        *   **T5 Base (`t5-base`):** A versatile text-to-text transfer transformer model. It can be used for various tasks, including summarization. *Limitations: Performance can vary depending on the specific task and fine-tuning, and like other large models, can be resource-intensive.*\n",
        "\n",
        "    *   **Question Answering Models:**\n",
        "        *   **DistilBERT SQuAD (`distilbert-base-uncased-distilled-squad`):** A distilled version of BERT, fine-tuned on the SQuAD dataset. It's faster and smaller than BERT while retaining much of its performance for extractive question answering. *Limitations: Primarily designed for extractive QA (finding answers within the text), may not perform well on questions requiring synthesis or external knowledge, and confidence scores should be interpreted as relative certainty within the model's knowledge.*\n",
        "        *   **BERT Base Cased SQuAD (`bert-base-cased-finetuned-squad`):** A base version of the BERT model, fine-tuned on the SQuAD dataset. Provides good performance on extractive QA. *Limitations: Similar to DistilBERT, primarily extractive, and larger than BERT.*\n",
        "\n",
        "    *   *Please note that all models have limitations, including potential biases present in their training data. Performance may vary depending on the complexity, domain, and length of the input text.*\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "             text_input = gr.Textbox(\n",
        "                 lines=10,\n",
        "                 placeholder=\"Paste or type your text here...\",\n",
        "                 label=\"Input Text\",\n",
        "                 interactive=True,\n",
        "             )\n",
        "             file_input = gr.File(label=\"Upload Text File (.txt, .pdf)\", type=\"filepath\")\n",
        "             url_input = gr.Textbox(\n",
        "                 lines=1,\n",
        "                 placeholder=\"Enter a URL here...\",\n",
        "                 label=\"Input from URL\",\n",
        "                 interactive=True,\n",
        "             )\n",
        "\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### Summarization Options\")\n",
        "            summarization_type_radio = gr.Radio(\n",
        "                [\"Abstractive\", \"Extractive\"],\n",
        "                label=\"Summarization Type\",\n",
        "                value=\"Abstractive\",\n",
        "                interactive=True\n",
        "            )\n",
        "            summary_length_sentences_slider = gr.Slider(\n",
        "                minimum=1,\n",
        "                maximum=20,\n",
        "                value=3,\n",
        "                step=1,\n",
        "                label=\"Summary Length (Sentences for Extractive)\",\n",
        "                interactive=True\n",
        "            )\n",
        "            max_length_slider = gr.Slider(\n",
        "                minimum=10,\n",
        "                maximum=500,\n",
        "                value=130,\n",
        "                step=10,\n",
        "                label=\"Max Summary Length (Tokens for Abstractive)\",\n",
        "                interactive=True\n",
        "            )\n",
        "            min_length_slider = gr.Slider(\n",
        "                minimum=5,\n",
        "                maximum=100,\n",
        "                value=30,\n",
        "                step=5,\n",
        "                label=\"Min Summary Length (Tokens for Abstractive)\",\n",
        "                interactive=True\n",
        "            )\n",
        "            summary_format_radio = gr.Radio(\n",
        "                [\"Paragraph\", \"Bullet Points\"],\n",
        "                label=\"Summary Format\",\n",
        "                value=\"Paragraph\",\n",
        "                interactive=True\n",
        "            )\n",
        "\n",
        "\n",
        "            gr.Markdown(\"### Model Selection\")\n",
        "            summarization_model_dropdown = gr.Dropdown(\n",
        "                choices=list(AVAILABLE_SUMMARIZATION_MODELS.keys()),\n",
        "                label=\"Select Summarization Model\",\n",
        "                value=\"BART Large CNN\",\n",
        "                interactive=True\n",
        "            )\n",
        "            qa_model_dropdown = gr.Dropdown(\n",
        "                choices=list(AVAILABLE_QA_MODELS.keys()),\n",
        "                label=\"Select QA Model\",\n",
        "                value=\"DistilBERT SQuAD\",\n",
        "                interactive=True\n",
        "            )\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(\n",
        "            lines=3,\n",
        "            placeholder=\"Enter your question(s) here (one per line)...\",\n",
        "            label=\"Ask a Question(s) (Optional)\",\n",
        "            scale=2,\n",
        "            interactive=True,\n",
        "        )\n",
        "        process_button = gr.Button(\n",
        "            \"Process Text and Questions\",\n",
        "            scale=1\n",
        "        )\n",
        "        clear_button = gr.Button(\n",
        "            \"Clear All\",\n",
        "            scale=1\n",
        "        )\n",
        "\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [\"This is a sample text about artificial intelligence. Artificial intelligence (AI) is the intelligence of machines or software. It is a field of study in computer science. AI machines can learn and solve problems. Common applications include natural language processing, speech recognition, and machine vision.\", None, None, \"What is AI?\\nWhere is AI used?\"],\n",
        "             [None, None, \"https://www.cnn.com/2024/06/27/politics/biden-debate-performance-analysis/index.html\", \"What is the article about?\"],\n",
        "        ],\n",
        "        inputs=[text_input, file_input, url_input, question_input],\n",
        "        label=\"Examples\"\n",
        "    )\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "        summary_output = gr.Textbox(\n",
        "            label=\"Summarized Text\",\n",
        "            interactive=False,\n",
        "            lines=10,\n",
        "            scale=1,\n",
        "            autoscroll=True\n",
        "        )\n",
        "        keywords_output = gr.Textbox(\n",
        "            label=\"Extracted Keywords\",\n",
        "            interactive=False,\n",
        "            lines=5,\n",
        "            scale=1,\n",
        "            autoscroll=True\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "         chatbot = gr.Chatbot(label=\"Conversation History and Answers\", scale=2, type='messages', show_copy_button=True)\n",
        "         error_display = gr.Textbox(label=\"Messages\", interactive=False, lines=2, scale=1, elem_id=\"error-message\", autoscroll=True)\n",
        "\n",
        "    # Adding output metrics display\n",
        "    gr.Markdown(\"### Output Metrics\")\n",
        "    with gr.Row():\n",
        "        input_word_count_output = gr.Textbox(label=\"Input Word Count\", interactive=False, scale=1)\n",
        "        summary_word_count_output = gr.Textbox(label=\"Summary Word Count\", interactive=False, scale=1)\n",
        "        compression_rate_output = gr.Textbox(label=\"Compression Rate (%)\", interactive=False, scale=1)\n",
        "        readability_score_output = gr.Textbox(label=\"Readability Score (Flesch Reading Ease)\", interactive=False, scale=1)\n",
        "\n",
        "\n",
        "    conversation_history_state = gr.State([])\n",
        "\n",
        "\n",
        "    process_button.click(\n",
        "        fn=summarize_and_answer_advanced,\n",
        "        inputs=[\n",
        "            text_input,\n",
        "            file_input,\n",
        "            url_input,\n",
        "            question_input,\n",
        "            summarization_type_radio,\n",
        "            summary_length_sentences_slider,\n",
        "            summary_format_radio,\n",
        "            summarization_model_dropdown,\n",
        "            qa_model_dropdown,\n",
        "            max_length_slider,\n",
        "            min_length_slider,\n",
        "            conversation_history_state\n",
        "        ],\n",
        "        outputs=[\n",
        "            summary_output,\n",
        "            keywords_output,\n",
        "            chatbot,\n",
        "            error_display,\n",
        "\n",
        "            input_word_count_output,\n",
        "            summary_word_count_output,\n",
        "            compression_rate_output,\n",
        "            readability_score_output\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    clear_button.click(\n",
        "        fn=clear_all,\n",
        "        inputs=[],\n",
        "        outputs=[\n",
        "            text_input,\n",
        "            file_input,\n",
        "            url_input,\n",
        "            question_input,\n",
        "            summary_output,\n",
        "            keywords_output,\n",
        "            chatbot,\n",
        "            error_display,\n",
        "            conversation_history_state,\n",
        "\n",
        "            input_word_count_output,\n",
        "            summary_word_count_output,\n",
        "            compression_rate_output,\n",
        "            readability_score_output\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "iface.launch(share=True) # Launching and to get a public URL\n",
        "\n",
        "print(\"To run this application, execute the code cell above. Gradio will provide a local URL (and a public URL if you set share=True) to access the interface in your browser.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://47b0b616cdcdba364a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://47b0b616cdcdba364a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To run this application, execute the code cell above. Gradio will provide a local URL (and a public URL if you set share=True) to access the interface in your browser.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20be9318"
      },
      "source": [
        "## Project Description\n",
        "\n",
        "This project is an advanced text processing application built using Gradio and various natural language processing libraries such as Hugging Face Transformers, RAKE-NLTK, NLTK, and TextStat. It provides a user-friendly web interface to perform the following tasks:\n",
        "\n",
        "- **Text Input:** Users can input text by pasting it directly into a textbox, uploading a `.txt` or `.pdf` file, or providing a URL for web scraping.\n",
        "- **Text Summarization:** The application supports both Abstractive (using models like BART Large CNN or T5 Base) and Extractive summarization. Users can control the length of the summary and choose between paragraph or bullet point format.\n",
        "- **Keyword Extraction:** Key phrases are extracted from the input text using RAKE-NLTK.\n",
        "- **Question Answering:** Users can ask questions based on the provided text using models like DistilBERT SQuAD or BERT Base Cased SQuAD. The application supports follow-up questions and provides confidence scores for the answers.\n",
        "- **Output Metrics:** Various metrics are calculated and displayed, including input word count, summary word count, compression rate, and readability score (Flesch Reading Ease).\n",
        "- **Model Selection:** Users can select their preferred models for summarization and question answering from available options.\n",
        "\n",
        "The application is designed to be interactive and provides clear instructions on how to use its features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Created and Submitted By:**\n",
        "**Debadatta Rout**\n",
        "\n",
        "------\n",
        "\n",
        "**Proxenix Internship**- Data Science & Analytics\n",
        "\n",
        "**Project Team** – Group 19\n",
        "\n",
        "# Details\n",
        "\n",
        "**Adarsh S J** [Team Leader]\n",
        "\n",
        "adarshsj4002@gmail.com\n",
        "\n",
        "8304907195\n",
        "\n",
        "------\n",
        "\n",
        "**Poonam Parida**\n",
        "\n",
        "poonamparida819@gmail.com\n",
        "\n",
        "8114965594\n",
        "\n",
        "**SOMYA RANJAN MAHAPATRA**\n",
        "\n",
        "srm270405@gmail.com\n",
        "\n",
        "7653869609\n",
        "\n",
        "**Debadatta Rout**\n",
        "\n",
        "routdebadatta22@gmail.com\n",
        "\n",
        "9827068499\n"
      ],
      "metadata": {
        "id": "jSxP2A3Qx5x0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UUhVaeebx8_p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}