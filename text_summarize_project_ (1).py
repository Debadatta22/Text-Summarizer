# -*- coding: utf-8 -*-
"""Text summarize project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1spdf4YuYqWIuOyNE-cgz4OodrLF9lO9C

# **My Setup**
"""

# Install Gradio for the web interface
!pip install gradio --quiet

# Install Hugging Face Transformers library
!pip install transformers --quiet

# Install RAKE-NLTK for keyword extraction
!pip install rake-nltk --quiet

# Install PDF handling library
!pip install pypdf --quiet

# Install BeautifulSoup and requests for web scraping
!pip install beautifulsoup4 requests --quiet

# Install TextStat for readability scoring
!pip install textstat --quiet

# Install NLTK and download required data
!pip install nltk --quiet
import nltk
nltk.download('punkt')
nltk.download('stopwords')

!pip install --upgrade transformers
!pip install gradio
!pip install newspaper3k
!pip install nltk
!pip install sumy
!pip install pdfplumber
!pip install python-docx
!pip install langdetect

# Commented out IPython magic to ensure Python compatibility.
# %pip install pypdf requests beautifulsoup4 -q

import nltk
nltk.download('stopwords')

# Commented out IPython magic to ensure Python compatibility.
# %pip install google-auth google-auth-oauthlib google-auth-httplib2 -q

# Commented out IPython magic to ensure Python compatibility.
# %pip install textstat -q

# Create requirements.txt
requirements = [
    "gradio",
    "transformers",
    "rake-nltk",
    "requests",
    "beautifulsoup4",
    "pypdf",
    "torch", # Often needed for transformers models
    "accelerate", # Can help with model loading and inference
    "textstat",
    "nltk" # Explicitly add nltk
]

with open("requirements.txt", "w") as f:
    for req in requirements:
        f.write(req + "\n")
print("Created requirements.txt")

import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

print("Downloading NLTK data (stopwords and punkt)...")
try:
    nltk.download('stopwords', quiet=True)
    print("NLTK stopwords downloaded.")
except Exception as e:
    print(f"Error downloading NLTK stopwords: {e}")

try:
    nltk.download('punkt', quiet=True)
    print("NLTK punkt tokenizer downloaded.")
except Exception as e:
    print(f"Error downloading NLTK punkt tokenizer: {e}")

print("NLTK data download attempts finished.")

# Commented out IPython magic to ensure Python compatibility.
# Install necessary libraries if not already installed
# %pip install gradio transformers rake-nltk requests beautifulsoup4 pypdf torch accelerate textstat nltk -q

import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

print("Downloading NLTK data (stopwords and punkt)...")
try:
    nltk.download('stopwords', quiet=True)
    print("NLTK stopwords downloaded.")
except Exception as e:
    print(f"Error downloading NLTK stopwords: {e}")

try:
    nltk.download('punkt', quiet=True)
    print("NLTK punkt tokenizer downloaded.")
except Exception as e:
    print(f"Error downloading NLTK punkt tokenizer: {e}")

print("NLTK data download attempts finished.")

import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

print("Downloading NLTK data (stopwords and punkt) explicitly...")
try:
    nltk.download('stopwords')
    print("NLTK stopwords downloaded.")
except Exception as e:
    print(f"Error downloading NLTK stopwords: {e}")

try:
    nltk.download('punkt')
    print("NLTK punkt tokenizer downloaded.")
except Exception as e:
    print(f"Error downloading NLTK punkt tokenizer: {e}")

print("NLTK data explicit download attempts finished.")

import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

print("Downloading NLTK data: punkt_tab...")
try:
    nltk.download('punkt_tab')
    print("NLTK punkt_tab downloaded.")
except Exception as e:
    print(f"Error downloading NLTK punkt_tab: {e}")

print("NLTK punkt_tab explicit download attempt finished.")

"""# **Code:**
execute or run the following tab for output or Summarization

"""

import gradio as gr
from transformers import pipeline
from rake_nltk import Rake
import requests
from bs4 import BeautifulSoup
from pypdf import PdfReader
import io
import tempfile
import sys
import traceback
import nltk
import textstat
import os

# Define available models
AVAILABLE_SUMMARIZATION_MODELS = {
    "BART Large CNN": "facebook/bart-large-cnn",
    "T5 Base": "t5-base", # Example of another model
}

AVAILABLE_QA_MODELS = {
    "DistilBERT SQuAD": "distilbert-base-uncased-distilled-squad",
    "BERT Base Cased SQuAD": "bert-base-cased-finetuned-squad", # Example of another model
}

# Cache models to avoid reloading on each call (already defined)
cached_pipelines = {}

def extract_text_from_pdf(pdf_file_path):
    """Extracts text from an uploaded PDF file."""
    text = ""
    try:
        # Ensure the file exists and is accessible
        if not pdf_file_path or not isinstance(pdf_file_path, str):
            return None, "Invalid PDF file input."
        if not tempfile.TemporaryDirectory().name in pdf_file_path and not os.path.exists(pdf_file_path):
             # Check if it's a temporary file path from Gradio or a standard path
            return None, f"PDF file not found: {pdf_file_path}"

        reader = PdfReader(pdf_file_path)
        if not reader.pages:
             return None, "PDF file contains no readable pages."

        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"

        if not text.strip():
             return None, "No text extracted from the PDF."

    except Exception as e:
        return None, f"Error reading PDF: {e}\n{traceback.format_exc()}"
    return text, None


def extract_text_from_url(url):
    """Extracts text content from a given URL."""
    text = ""
    if not url or not isinstance(url, str):
        return None, "Invalid URL input."

    try:
        response = requests.get(url, timeout=10) # Added timeout
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # Improved text extraction: try different strategies
        extracted_text = ""
        # Strategy 1: Find common article/main content tags
        main_content = soup.find('article') or soup.find('main')
        if main_content:
             text_elements = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']) # Added list items
             extracted_text = ' '.join(elem.get_text() for elem in text_elements if elem.get_text())

        # Strategy 2: If not much found, try extracting text from common block elements
        if not extracted_text or len(extracted_text.split()) < 50: # If limited text from main content
             block_elements = soup.find_all(['p', 'div', 'blockquote', 'li']) # Added div, blockquote
             extracted_text = ' '.join(elem.get_text() for elem in block_elements if elem.get_text())


        # Strategy 3: Fallback to getting all text if still not enough
        if not extracted_text or len(extracted_text.split()) < 50:
             extracted_text = soup.get_text()


        text = extracted_text

        # Basic cleaning: remove excessive whitespace and potential script/style text
        for script_or_style in soup(["script", "style"]):
            script_or_style.extract() # Remove these tags

        text = ' '.join(text.split()) # Normalize whitespace

        if not text.strip():
             return None, "No meaningful text extracted from the URL."


    except requests.exceptions.Timeout:
        return None, f"Error fetching URL: Request timed out after 10 seconds."
    except requests.exceptions.RequestException as e:
        return None, f"Error fetching URL: {e}\n{traceback.format_exc()}"
    except Exception as e:
        return None, f"Error parsing URL content: {e}\n{traceback.format_exc()}"
    return text, None


def get_pipeline(task, model_name):
    """Gets or creates a cached pipeline for a given task and model."""
    cache_key = f"{task}_{model_name}"
    if not model_name:
         return None, f"No model selected for {task}."

    if cache_key not in cached_pipelines:
        try:
            print(f"Loading {task} model: {model_name}")
            # Add device mapping for potentially better performance if GPU available
            try:
                import torch
                device = 0 if torch.cuda.is_available() else -1
            except ImportError:
                device = -1 # Default to CPU if torch is not available or CUDA is not

            if task == "summarization":
                 cached_pipelines[cache_key] = pipeline(task, model=model_name, device=device)
            elif task == "question-answering":
                 cached_pipelines[cache_key] = pipeline(task, model=model_name, return_all_scores=True, device=device)
            else:
                 cached_pipelines[cache_key] = pipeline(task, model=model_name, device=device)
            print(f"Successfully loaded {model_name} for {task}.")
        except Exception as e:
            print(f"Error loading model {model_name} for {task}: {e}\n{traceback.format_exc()}")
            # Remove the failed model from cache if it was partially added
            if cache_key in cached_pipelines:
                 del cached_pipelines[cache_key]
            return None, f"Error loading model: {model_name}. Details: {e}"
    else:
        print(f"Using cached model: {model_name} for {task}")
    return cached_pipelines.get(cache_key), None


def summarize_and_answer_advanced(
    text_input_box, file_input, url_input, question, summarization_type,
    summary_length_sentences, summary_format,
    summarization_model_key, qa_model_key,
    max_length, min_length,
    history
):
    """
    Handles input from textbox, file upload, or URL, summarizes the text,
    extracts keywords, and answers questions based on the text, with advanced
    summarization options, supporting follow-up questions and confidence scores.
    Allows model selection and parameter tuning, and summary format selection.
    Calculates and returns output metrics.
    Includes basic error handling and prepares outputs for download.
    Implements caching for models and handles large text inputs.
    """
    text = ""
    summary = ""
    keywords = []
    error_message = ""
    new_history = history
    download_summary = ""
    download_qa = ""

    # Metrics initialization
    input_word_count = 0
    summary_word_count = 0
    compression_rate = 0.0
    readability_score = 0.0


    MAX_TEXT_SIZE_BYTES = 1 * 1024 * 1024 # 1 MB limit for input text to prevent memory issues

    # --- Input Handling ---
    # Prioritizing input sources: File > URL > Textbox
    if file_input is not None:
        # Gradio's File component provides a file path
        file_path = file_input # Gradio passes the file path as a string
        if file_path: # Ensure file_path is not None or empty
            if file_path.lower().endswith('.pdf'):
                text, error_message = extract_text_from_pdf(file_path)
            elif file_path.lower().endswith('.txt'):
                 try:
                     with open(file_path, 'r', encoding='utf-8') as f:
                         text = f.read()
                 except Exception as e:
                     text = None
                     error_message = f"Error reading text file: {e}\n{traceback.format_exc()}"
            else:
                 text = None
                 error_message = "Unsupported file type. Please upload a .txt or .pdf file."
        else:
             text = None
             error_message = "File input is present but file path is missing."


    elif url_input:
        text, error_message = extract_text_from_url(url_input)

    elif text_input_box:
        text = text_input_box


    # Calculating input word count immediately after getting the text
    if text:
        input_word_count = len(text.split())

    if error_message:
         # Ensuring all outputs are returned even with an error
         return summary, "; ".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score

    if not text:
        error_message = "Please provide text via the textbox, file upload, or URL."
        return summary, "; ".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score

    # Checking text size
    if sys.getsizeof(text) > MAX_TEXT_SIZE_BYTES:
        error_message = f"Input text size exceeds the limit of {MAX_TEXT_SIZE_BYTES / (1024*1024):.2f} MB. Please provide a smaller text."
        return summary, "; ".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score


    # Getting the selected summarization model pipeline
    summarization_model_name = AVAILABLE_SUMMARIZATION_MODELS.get(summarization_model_key)
    summarizer, model_load_error = get_pipeline("summarization", summarization_model_name)
    if model_load_error:
        error_message += f"\n{model_load_error}"
        return summary, "; ".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score

    # Getting the selected QA model pipeline
    qa_model_name = AVAILABLE_QA_MODELS.get(qa_model_key)
    question_answerer, model_load_error = get_pipeline("question-answering", qa_model_name)
    if model_load_error:
        error_message += f"\n{model_load_error}"
        return summary, "; ".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score


    # --- Summarization and Keyword Extraction ---
    try:
        if summarization_type == "Abstractive" and summarizer:

            summarizer_output = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
            if summarizer_output and isinstance(summarizer_output, list) and len(summarizer_output) > 0 and 'summary_text' in summarizer_output[0]:
                 summary = summarizer_output[0]['summary_text']
            else:
                 summary = "Could not generate abstractive summary."
                 print(f"Abstractive summarizer returned unexpected output: {summarizer_output}")
                 if not error_message: # Add to error message only if no prior error
                      error_message = "Abstractive summarizer failed."
                 else:
                      error_message += "\nAbstractive summarizer failed."


        elif summarization_type == "Extractive":
            # Using NLTK's sent_tokenize for more accurate sentence splitting
            try:
                sentences = nltk.sent_tokenize(text)
                sentences = [s.strip() for s in sentences if s.strip()]
                if not sentences:
                     summary = "Could not extract sentences for extractive summary."
                     if not error_message:
                          error_message = "Extractive summarizer failed."
                else:
                     num_sentences = min(len(sentences), summary_length_sentences)
                     summary = '. '.join(sentences[:num_sentences])
            except LookupError:
                 summary = "Extractive summarization failed (NLTK punkt missing)."
                 if not error_message: error_message = ""
                 error_message += "\nNLTK Punkt tokenizer not found. Please ensure it's downloaded."
            except Exception as e:
                 summary = "Error during extractive summarization."
                 if not error_message: error_message = ""
                 error_message += f"\nError during extractive summarization: {e}\n{traceback.format_exc()}"


        elif not summarizer:
             summary = "Summarization model not loaded."
             if not error_message:
                  error_message = "Summarization model not loaded."


        # Applying summary format
        if summary and summary_format == "Bullet Points":
            # Usng NLTK's sent_tokenize for more accurate sentence splitting for bullet points
            try:
                sentences_for_bullets = nltk.sent_tokenize(summary)
                # Filtering out empty strings and strip whitespace, then add bullet points
                bullet_points = [f"- {s.strip()}" for s in sentences_for_bullets if s.strip()]
                summary = "\n".join(bullet_points)
            except LookupError:
                 # This error should ideally be caught during initial NLTK download check, but as a fallback
                 error_message += "\nNLTK Punkt tokenizer not found. Cannot format summary as bullet points."
                 # summary remains in paragraph format
            except Exception as e:
                 error_message += f"\nError formatting summary as bullet points: {e}\n{traceback.format_exc()}"
                 # Fallback to original summary if formatting fails
                 pass


        # Calculation for summary word count (after formatting)
        if summary:
            summary_word_count = len(summary.split())

        # Calculation for compression rate
        if input_word_count > 0:
            compression_rate = ((input_word_count - summary_word_count) / input_word_count) * 100
            compression_rate = max(0.0, compression_rate)
        else:
             compression_rate = 0.0

        # Calculation for readability score
        if text:
            try:
                if len(text.split()) >= 50:
                     readability_score = textstat.flesch_reading_ease(text)
                else:
                     readability_score = -2.0 # Indicate text is too short
                     if not error_message: error_message = ""
                     error_message += "\nNote: Readability score may be unreliable for texts shorter than 50 words."

            except Exception as re:
                readability_score = -1.0 # Indicate error
                print(f"Error calculating readability score: {re}\n{traceback.format_exc()}")
                if not error_message: error_message = ""
                error_message += f"\nError calculating readability score: {re}"


        # Keyword Extraction
        try:

            try:
                nltk.data.find('tokenizers/punkt')
                nltk.data.find('corpora/stopwords')
            except LookupError:
                 print("NLTK data not found. Attempting re-download within function...")
                 try:
                     import ssl
                     try:
                         _create_unverified_https_context = ssl._create_unverified_context
                     except AttributeError:
                         pass
                     else:
                         ssl._create_default_https_context = _create_unverified_https_context
                     nltk.download('punkt', quiet=True)
                     nltk.download('stopwords', quiet=True)
                     print("NLTK data re-downloaded.")
                 except Exception as e:
                     print(f"Error during NLTK re-download: {e}")


            r = Rake()
            r.extract_keywords_from_text(text)
            keywords = r.get_ranked_phrases()[:10]
            if not keywords:
                 keywords = ["No keywords extracted."]
        except Exception as ke:
             keywords = ["Error extracting keywords. Please check NLTK data."]
             print(f"Keyword Extraction Error: {ke}\n{traceback.format_exc()}")
             if not error_message: error_message = ""
             error_message += f"\nError extracting keywords: {ke}"


    except Exception as e:
        summary = "Error during summarization or keyword extraction."
        keywords = ["Error during processing."]
        error_message = f"Processing Error during summarization/keywords: {e}\n{traceback.format_exc()}"
        return summary, "; ".join(keywords), new_history, error_message, download_summary, download_qa, input_word_count, summary_word_count, compression_rate, readability_score
def download_file(content, filename):
    if content:
        try:
            with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".txt", encoding='utf-8') as tmp_file:
                tmp_file.write(content)
                tmp_file_path = tmp_file.name
            return gr.File(value=tmp_file_path, filename=filename, visible=True)

        except Exception as e:
            print(f"Error creating temporary file for download: {e}\n{traceback.format_exc()}")
            return None
    else:
        return None


def update_download_buttons(summary_content, qa_content):
    summary_interactive = bool(summary_content)
    qa_interactive = bool(qa_content)
    return gr.update(interactive=summary_interactive), gr.update(interactive=qa_interactive)


def clear_all():
    return (
        gr.update(value=""), # text_input
        gr.update(value=None), # file_input
        gr.update(value=""), # url_input
        gr.update(value=""), # question_input
        gr.update(value=""), # summary_output
        gr.update(value=""), # keywords_output
        None, # chatbot
        gr.update(value=""), # error_display
        [], # conversation_history_state
        "",
        "",
        gr.update(interactive=False), # download_summary_button
        gr.update(interactive=False), # download_qa_button
        gr.update(value=""), # input_word_count_output
        gr.update(value=""), # summary_word_count_output
        gr.update(value=""), # compression_rate_output
        gr.update(value="") # readability_score_output
    )


# Creating the Gradio interface with all features integrated
with gr.Blocks(theme=gr.themes.Soft(), title="Text Processing App") as iface:
    gr.Markdown("""
    # Welcome to our advanced Text Processing App!
    Summarize text, extract keywords, and get answers to your questions.
    Provide text via the textbox, upload a file (.txt, .pdf), or enter a URL.
    """)


    # Adding instructions section
    gr.Markdown("""
    ## How to Use:

    1.  **Input Text:** Provide the text you want to process by:
        *   Typing or pasting it directly into the "Input Text" box.
        *   Uploading a `.txt` or `.pdf` file using the "Upload Text File" option.
        *   Entering a URL in the "Input from URL" box (the application will attempt to scrape the main text content).
        *   *Note: Only one input method is needed. File upload and URL input take precedence over the textbox.*
        *   *Input text size is limited to approximately 1 MB to prevent performance issues.*

    2.  **Summarization Options:**
        *   Choose "Abstractive" or "Extractive" summarization.
        *   For Abstractive, use the sliders to set the desired "Max Summary Length" and "Min Summary Length" in tokens.
        *   For Extractive, use the slider to set the desired "Summary Length" in sentences.
        *   **Summary Format:** Choose between "Paragraph" or "Bullet Points".

    3.  **Model Selection:**
        *   Select your preferred Summarization Model from the dropdown.
        *   Select your preferred Question Answering Model from the dropdown.

    4.  **Ask Questions:**
        *   Enter your questions in the "Ask a Question(s)" box. You can ask multiple questions by typing each on a new line.
        *   The application will attempt to answer questions based on the provided input text.
        *   Follow-up questions are supported as the conversation history is used as additional context.

    5.  **Process:** Click the "Process Text and Questions" button to run the summarization, keyword extraction, and question answering.

    6.  **View Results:**
        *   The "Summarized Text" box will show the generated summary.
        *   The "Extracted Keywords" box will list key phrases from the text, separated by semicolons.
        *   The "Conversation History and Answers" chatbot will display your questions and the model's answers, including a confidence score for each answer.
        *   Any messages or errors will appear in the "Messages" box.
        *   **Output Metrics:** See the calculated metrics below the output areas.

    7.  **Download Results:**
        *   Click "Download Summary" to save the summarized text as a .txt file.
        *   Click "Download Q&A" to save the conversation history and answers as a .txt file.
        *   *Download buttons become active after processing.*

    8.  **Clear:** Click the "Clear All" button to reset all input and output fields.

    ## About the Models:

    This application uses transformer models from the Hugging Face library.

    *   **Summarization Models:**
        *   **BART Large CNN (`facebook/bart-large-cnn`):** A powerful abstractive summarization model fine-tuned on the CNN/DailyMail dataset. It is known for generating coherent and fluent summaries. *Limitations: Can be computationally intensive, may struggle with very long documents (exceeding its context window), and might occasionally hallucinate details not present in the source text.*
        *   **T5 Base (`t5-base`):** A versatile text-to-text transfer transformer model. It can be used for various tasks, including summarization. *Limitations: Performance can vary depending on the specific task and fine-tuning, and like other large models, can be resource-intensive.*

    *   **Question Answering Models:**
        *   **DistilBERT SQuAD (`distilbert-base-uncased-distilled-squad`):** A distilled version of BERT, fine-tuned on the SQuAD dataset. It's faster and smaller than BERT while retaining much of its performance for extractive question answering. *Limitations: Primarily designed for extractive QA (finding answers within the text), may not perform well on questions requiring synthesis or external knowledge, and confidence scores should be interpreted as relative certainty within the model's knowledge.*
        *   **BERT Base Cased SQuAD (`bert-base-cased-finetuned-squad`):** A base version of the BERT model, fine-tuned on the SQuAD dataset. Provides good performance on extractive QA. *Limitations: Similar to DistilBERT, primarily extractive, and larger than BERT.*

    *   *Please note that all models have limitations, including potential biases present in their training data. Performance may vary depending on the complexity, domain, and length of the input text.*

    """)

    with gr.Row():
        with gr.Column(scale=2):
             text_input = gr.Textbox(
                 lines=10,
                 placeholder="Paste or type your text here...",
                 label="Input Text",
                 interactive=True,
             )
             file_input = gr.File(label="Upload Text File (.txt, .pdf)", type="filepath")
             url_input = gr.Textbox(
                 lines=1,
                 placeholder="Enter a URL here...",
                 label="Input from URL",
                 interactive=True,
             )


        with gr.Column(scale=1):
            gr.Markdown("### Summarization Options")
            summarization_type_radio = gr.Radio(
                ["Abstractive", "Extractive"],
                label="Summarization Type",
                value="Abstractive",
                interactive=True
            )
            summary_length_sentences_slider = gr.Slider(
                minimum=1,
                maximum=20,
                value=3,
                step=1,
                label="Summary Length (Sentences for Extractive)",
                interactive=True
            )
            max_length_slider = gr.Slider(
                minimum=10,
                maximum=500,
                value=130,
                step=10,
                label="Max Summary Length (Tokens for Abstractive)",
                interactive=True
            )
            min_length_slider = gr.Slider(
                minimum=5,
                maximum=100,
                value=30,
                step=5,
                label="Min Summary Length (Tokens for Abstractive)",
                interactive=True
            )
            summary_format_radio = gr.Radio(
                ["Paragraph", "Bullet Points"],
                label="Summary Format",
                value="Paragraph",
                interactive=True
            )


            gr.Markdown("### Model Selection")
            summarization_model_dropdown = gr.Dropdown(
                choices=list(AVAILABLE_SUMMARIZATION_MODELS.keys()),
                label="Select Summarization Model",
                value="BART Large CNN",
                interactive=True
            )
            qa_model_dropdown = gr.Dropdown(
                choices=list(AVAILABLE_QA_MODELS.keys()),
                label="Select QA Model",
                value="DistilBERT SQuAD",
                interactive=True
            )


    with gr.Row():
        question_input = gr.Textbox(
            lines=3,
            placeholder="Enter your question(s) here (one per line)...",
            label="Ask a Question(s) (Optional)",
            scale=2,
            interactive=True,
        )
        process_button = gr.Button(
            "Process Text and Questions",
            scale=1
        )
        clear_button = gr.Button(
            "Clear All",
            scale=1
        )


    gr.Examples(
        examples=[
            ["This is a sample text about artificial intelligence. Artificial intelligence (AI) is the intelligence of machines or software. It is a field of study in computer science. AI machines can learn and solve problems. Common applications include natural language processing, speech recognition, and machine vision.", None, None, "What is AI?\nWhere is AI used?"],
             [None, None, "https://www.cnn.com/2024/06/27/politics/biden-debate-performance-analysis/index.html", "What is the article about?"],
        ],
        inputs=[text_input, file_input, url_input, question_input],
        label="Examples"
    )


    with gr.Row():
        summary_output = gr.Textbox(
            label="Summarized Text",
            interactive=False,
            lines=10,
            scale=1,
            autoscroll=True
        )
        keywords_output = gr.Textbox(
            label="Extracted Keywords",
            interactive=False,
            lines=5,
            scale=1,
            autoscroll=True
        )

    with gr.Row():
         chatbot = gr.Chatbot(label="Conversation History and Answers", scale=2, type='messages', show_copy_button=True)
         error_display = gr.Textbox(label="Messages", interactive=False, lines=2, scale=1, elem_id="error-message", autoscroll=True)

    # Adding output metrics display
    gr.Markdown("### Output Metrics")
    with gr.Row():
        input_word_count_output = gr.Textbox(label="Input Word Count", interactive=False, scale=1)
        summary_word_count_output = gr.Textbox(label="Summary Word Count", interactive=False, scale=1)
        compression_rate_output = gr.Textbox(label="Compression Rate (%)", interactive=False, scale=1)
        readability_score_output = gr.Textbox(label="Readability Score (Flesch Reading Ease)", interactive=False, scale=1)


    conversation_history_state = gr.State([])


    process_button.click(
        fn=summarize_and_answer_advanced,
        inputs=[
            text_input,
            file_input,
            url_input,
            question_input,
            summarization_type_radio,
            summary_length_sentences_slider,
            summary_format_radio,
            summarization_model_dropdown,
            qa_model_dropdown,
            max_length_slider,
            min_length_slider,
            conversation_history_state
        ],
        outputs=[
            summary_output,
            keywords_output,
            chatbot,
            error_display,

            input_word_count_output,
            summary_word_count_output,
            compression_rate_output,
            readability_score_output
        ]
    )




    clear_button.click(
        fn=clear_all,
        inputs=[],
        outputs=[
            text_input,
            file_input,
            url_input,
            question_input,
            summary_output,
            keywords_output,
            chatbot,
            error_display,
            conversation_history_state,

            input_word_count_output,
            summary_word_count_output,
            compression_rate_output,
            readability_score_output
        ]
    )


iface.launch(share=True) # Launching and to get a public URL

print("To run this application, execute the code cell above. Gradio will provide a local URL (and a public URL if you set share=True) to access the interface in your browser.")

"""## Project Description

This project is an advanced text processing application built using Gradio and various natural language processing libraries such as Hugging Face Transformers, RAKE-NLTK, NLTK, and TextStat. It provides a user-friendly web interface to perform the following tasks:

- **Text Input:** Users can input text by pasting it directly into a textbox, uploading a `.txt` or `.pdf` file, or providing a URL for web scraping.
- **Text Summarization:** The application supports both Abstractive (using models like BART Large CNN or T5 Base) and Extractive summarization. Users can control the length of the summary and choose between paragraph or bullet point format.
- **Keyword Extraction:** Key phrases are extracted from the input text using RAKE-NLTK.
- **Question Answering:** Users can ask questions based on the provided text using models like DistilBERT SQuAD or BERT Base Cased SQuAD. The application supports follow-up questions and provides confidence scores for the answers.
- **Output Metrics:** Various metrics are calculated and displayed, including input word count, summary word count, compression rate, and readability score (Flesch Reading Ease).
- **Model Selection:** Users can select their preferred models for summarization and question answering from available options.

The application is designed to be interactive and provides clear instructions on how to use its features.

# **Created and Submitted By:**
**Debadatta Rout**

------

**Proxenix Internship**- Data Science & Analytics

**Project Team** – Group 19

# Details

**Adarsh S J** [Team Leader]

adarshsj4002@gmail.com

8304907195

------

**Poonam Parida**

poonamparida819@gmail.com

8114965594

**SOMYA RANJAN MAHAPATRA**

srm270405@gmail.com

7653869609

**Debadatta Rout**

routdebadatta22@gmail.com

9827068499
"""

